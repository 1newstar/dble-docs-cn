## 2.22 KILL @@DDL_LOCK

### 背景
在运维dble集群的过程中，有时会遇到 " There is other session is doing DDL " 或者 "xxx is doing DDL " 等问题，这些问题会导致某些表无法被操作，无法进行 reload 等。此时，需要人工干预保证集群运行正常。

### 逻辑判断 

可通过 ZK 键值判断执行 DDL 中的节点状态：

通过键 /dble/{cluster-id}/ddl/{schema.table} 的值内容判断，该值理论上只会被发起者节点所修改，值的内容为json格式，其中有一个 status 字段：

status 为 INIT 时，发起动作的节点会锁住本节点对应 table 的 table meta，获取当前key的分布式锁。其他的dble节点在本阶段只会锁住本节点对应 table 的 table meta； 

发起节点会根据 DDL 执行结果更新当前键的值，这时根据 DDL 执行结果 status 分为两种情形：SUCCESS 和 FAILED。当状态被更新为为SUCCESS时，其他节点才会真正执行对应的DDL操作，否则则会取消执行并释放本节点对应table 的 table meta 锁；

插入一条/dble/{sponsor-cluster-id}/ddl/{schema.table}/instance/{sponsor-dble-id}:SUCCESS

到此，发起者节点已经结束，但是发起者节点会等待其他节点响应，只有全部节点都汇报执行完成之后，当前ddl才算完成。

发起者节点通过什么来判断其他节点都响应完成？

/dble/{sponsor-cluster-id}/ddl/{schema.table}/instance/下属结点

/dble/{sponsor-cluster-id}/online/下属结点

dble 通过 2 下面online节点的数量来判定1中发起的动作是否被所有人执行，只有1和2的 下属节点符合时，ddl才算真正意义上的完成。此时，发起者会释放 table meta 锁 和 分布式锁，并且删除 dble/{cluster-id}/ddl/{schema.table} kv 树。
 

### kill ddl_lock
当前指令只会释放对应ddl在当前节点中所持有的元数据锁。
若出现上述问题，可以参考 上面如何判断其他节点都响应完成的方式找出哪些没有响应，在这些节点和发起节点上执行当前命令，并手动删除dble/{cluster-id}/ddl/{schema.table} kv 树。

注意：如果不是特殊因素，此命令请不要随意使用。
 


